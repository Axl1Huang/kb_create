#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„æ‰¹å¤„ç†å…¥å£
"""
import sys
import os
import time
import json
import logging
from pathlib import Path
from typing import Optional, Dict, List
import argparse
import gc

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from src.core.config import UnifiedConfig, setup_logging
from src.core.pdf_processor import PDFProcessor
from src.core.llm_parser import LLMParser
from src.core.data_importer import DataImporter
from src.core.database import DatabaseManager

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    def __init__(self):
        self.metrics = {}
        self.start_time = time.time()

    def record_metric(self, name, value):
        if name not in self.metrics:
            self.metrics[name] = []
        self.metrics[name].append({
            "timestamp": time.time(),
            "value": value
        })

    def calculate_throughput(self):
        elapsed = time.time() - self.start_time
        if elapsed > 0:
            # è®¡ç®—å¤„ç†çš„PDFæ•°é‡ï¼ˆå‡è®¾è¿™æ˜¯ä¸»è¦æŒ‡æ ‡ï¼‰
            pdf_processed = len(self.metrics.get("pdf_processed", []))
            return pdf_processed / elapsed
        return 0

    def get_resource_usage(self):
        import psutil
        process = psutil.Process(os.getpid())
        return {
            "memory_mb": process.memory_info().rss / 1024 / 1024,
            "cpu_percent": process.cpu_percent()
        }

    def identify_bottlenecks(self):
        # ç®€å•çš„ç“¶é¢ˆè¯†åˆ«é€»è¾‘
        bottlenecks = []
        if self.metrics.get("pdf_processing_time"):
            avg_time = sum([m["value"] for m in self.metrics["pdf_processing_time"]]) / len(self.metrics["pdf_processing_time"])
            if avg_time > 60:  # å¦‚æœå¹³å‡å¤„ç†æ—¶é—´è¶…è¿‡60ç§’
                bottlenecks.append("PDFå¤„ç†æ—¶é—´è¿‡é•¿")

        if self.metrics.get("md_parsing_time"):
            avg_time = sum([m["value"] for m in self.metrics["md_parsing_time"]]) / len(self.metrics["md_parsing_time"])
            if avg_time > 30:  # å¦‚æœå¹³å‡è§£ææ—¶é—´è¶…è¿‡30ç§’
                bottlenecks.append("MDè§£ææ—¶é—´è¿‡é•¿")

        return bottlenecks

    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            "processing_time": time.time() - self.start_time,
            "throughput": self.calculate_throughput(),
            "resource_usage": self.get_resource_usage(),
            "bottlenecks": self.identify_bottlenecks(),
            "metrics": self.metrics
        }
        return report

class MemoryManagedProcessor:
    """å†…å­˜ç®¡ç†å¤„ç†å™¨"""
    def __init__(self, config: UnifiedConfig):
        self.config = config
        self.memory_threshold = self.calculate_memory_threshold()

    def calculate_memory_threshold(self):
        """è®¡ç®—å†…å­˜é˜ˆå€¼"""
        import psutil
        total_memory = psutil.virtual_memory().total
        # è®¾ç½®ä¸ºæ€»å†…å­˜çš„80%
        return total_memory * 0.8

    def memory_usage_exceeds_threshold(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æ˜¯å¦è¶…è¿‡é˜ˆå€¼"""
        import psutil
        current_memory = psutil.virtual_memory().used
        return current_memory > self.memory_threshold

    def cleanup_unused_resources(self):
        """æ¸…ç†æœªä½¿ç”¨çš„èµ„æº"""
        # æ¸…ç†ç¼“å­˜
        gc.collect()
        # å¯ä»¥æ·»åŠ æ›´å¤šæ¸…ç†é€»è¾‘

    def process_task(self, task):
        """å¤„ç†ä»»åŠ¡"""
        if self.memory_usage_exceeds_threshold():
            self.cleanup_unused_resources()
        return task()

class UnifiedBatchProcessor:
    """ç»Ÿä¸€çš„æ‰¹å¤„ç†å™¨"""
    def __init__(self, config: UnifiedConfig):
        self.config = config
        self.pdf_processor = PDFProcessor(config)
        self.llm_parser = LLMParser(config)
        self.data_importer = DataImporter(config)
        self.performance_monitor = PerformanceMonitor()
        self.memory_manager = MemoryManagedProcessor(config)

        # è®¾ç½®æ—¥å¿—
        log_file = config.paths.logs_dir / "unified_batch_processor.log"
        self.logger = setup_logging(log_file, "INFO")

    def process_pdfs(self, limit: Optional[int] = None, workers: Optional[int] = None):
        """å¤„ç†PDFæ–‡ä»¶"""
        input_dir = self.config.paths.input_dir
        output_dir = self.config.paths.output_dir / "markdown"

        # ä¸´æ—¶ä¿®æ”¹å·¥ä½œçº¿ç¨‹æ•°
        original_workers = self.config.parallel.pdf_max_workers
        if workers:
            self.config.parallel.pdf_max_workers = workers

        try:
            self.logger.info(f"å¼€å§‹å¤„ç†PDFæ–‡ä»¶ï¼Œè¾“å…¥ç›®å½•: {input_dir}")
            results = self.pdf_processor.process_batch(input_dir, output_dir, limit=limit)
            self.logger.info(f"PDFå¤„ç†å®Œæˆ: {results}")
            return results
        finally:
            # æ¢å¤åŸå§‹å·¥ä½œçº¿ç¨‹æ•°
            self.config.parallel.pdf_max_workers = original_workers

    def parse_mds(self, limit: Optional[int] = None):
        """è§£æMDæ–‡ä»¶"""
        input_dir = self.config.paths.output_dir / "markdown"

        if not input_dir.exists():
            self.logger.error(f"Markdownç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return {"parsed": 0, "failed": 0, "errors": []}

        # è·å–æ‰€æœ‰markdownæ–‡ä»¶
        md_files = list(input_dir.glob("*.md"))
        if limit:
            md_files = md_files[:limit]

        if not md_files:
            self.logger.warning("æœªæ‰¾åˆ°Markdownæ–‡ä»¶")
            return {"parsed": 0, "failed": 0, "errors": []}

        results = {"parsed": 0, "failed": 0, "errors": []}

        for md_file in md_files:
            try:
                start_time = time.time()
                parsed_data = self.llm_parser.parse_markdown_file(str(md_file))
                parse_time = time.time() - start_time

                if parsed_data and parsed_data.get("title"):
                    results["parsed"] += 1
                    self.performance_monitor.record_metric("md_parsing_time", parse_time)
                    self.logger.info(f"æˆåŠŸè§£æMDæ–‡ä»¶: {md_file.name}")
                else:
                    results["failed"] += 1
                    results["errors"].append(str(md_file))
                    self.logger.warning(f"MDæ–‡ä»¶è§£æç»“æœä¸å®Œæ•´: {md_file.name}")
            except Exception as e:
                results["failed"] += 1
                results["errors"].append(str(md_file))
                self.logger.error(f"è§£æMDæ–‡ä»¶å¤±è´¥ {md_file.name}: {e}")

        self.logger.info(f"MDè§£æå®Œæˆ: æˆåŠŸ {results['parsed']}, å¤±è´¥ {results['failed']}")
        return results

    def import_data(self, limit: Optional[int] = None):
        """å¯¼å…¥æ•°æ®"""
        input_dir = self.config.paths.output_dir / "markdown"

        if not input_dir.exists():
            self.logger.error(f"Markdownç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return {"imported": 0, "failed": 0, "errors": []}

        # è·å–æ‰€æœ‰markdownæ–‡ä»¶
        md_files = list(input_dir.glob("*.md"))
        if limit:
            md_files = md_files[:limit]

        if not md_files:
            self.logger.warning("æœªæ‰¾åˆ°Markdownæ–‡ä»¶")
            return {"imported": 0, "failed": 0, "errors": []}

        results = self.data_importer.import_batch(md_files)
        self.logger.info(f"æ•°æ®å¯¼å…¥å®Œæˆ: æˆåŠŸ {results['imported']}, å¤±è´¥ {results['failed']}")
        return results

    def run_full_pipeline(self, limit: Optional[int] = None, workers: Optional[int] = None):
        """è¿è¡Œå®Œæ•´ç®¡é“"""
        self.logger.info("=== å¼€å§‹å®Œæ•´å¤„ç†ç®¡é“ ===")

        final_results = {
            "pdf_processing": None,
            "md_parsing": None,
            "data_import": None,
            "success": True
        }

        try:
            # PDFå¤„ç†é˜¶æ®µ
            pdf_results = self.process_pdfs(limit=limit, workers=workers)
            final_results["pdf_processing"] = pdf_results

            # MDè§£æé˜¶æ®µ
            md_results = self.parse_mds(limit=limit)
            final_results["md_parsing"] = md_results

            # æ•°æ®å¯¼å…¥é˜¶æ®µ
            import_results = self.import_data(limit=limit)
            final_results["data_import"] = import_results

            self.logger.info("=== å®Œæ•´å¤„ç†ç®¡é“å®Œæˆ ===")
            return final_results

        except Exception as e:
            self.logger.error(f"ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
            final_results["success"] = False
            final_results["error"] = str(e)
            return final_results

    def generate_performance_report(self, output_file: Optional[Path] = None):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = self.performance_monitor.generate_report()

        if not output_file:
            output_file = self.config.paths.logs_dir / f"performance_report_{int(time.time())}.json"

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        self.logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
        return report

def main():
    parser = argparse.ArgumentParser(
        description="ç»Ÿä¸€çš„æ‰¹å¤„ç†å…¥å£",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python unified_batch_processor.py                          # å®Œæ•´å¤„ç†æµç¨‹
  python unified_batch_processor.py --mode pdf_only          # åªå¤„ç†PDF
  python unified_batch_processor.py --mode import_only       # åªå¯¼å…¥æ•°æ®
  python unified_batch_processor.py --log-level DEBUG        # è°ƒè¯•æ¨¡å¼è¿è¡Œ
        """
    )

    parser.add_argument(
        "--mode",
        choices=["full", "pdf_only", "parse_only", "import_only"],
        default="full",
        help="å¤„ç†æ¨¡å¼ï¼ˆé»˜è®¤: fullï¼‰"
    )

    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="è®¾ç½®æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤: INFOï¼‰"
    )

    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="é™åˆ¶å¤„ç†çš„æ–‡ä»¶æ•°é‡"
    )

    parser.add_argument(
        "--workers",
        type=int,
        default=None,
        help="è®¾ç½®PDFå¤„ç†çš„å·¥ä½œçº¿ç¨‹æ•°"
    )

    parser.add_argument(
        "--config",
        type=Path,
        help="æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„"
    )

    parser.add_argument(
        "--input-dir",
        type=Path,
        help="æŒ‡å®šè¾“å…¥ç›®å½•ï¼ˆè¦†ç›–é…ç½®ï¼‰"
    )

    parser.add_argument(
        "--output-report",
        type=Path,
        help="æŒ‡å®šæ€§èƒ½æŠ¥å‘Šè¾“å‡ºè·¯å¾„"
    )

    args = parser.parse_args()

    try:
        # åŠ è½½é…ç½®
        config = UnifiedConfig(config_path=args.config)

        # å¦‚æœæŒ‡å®šäº†è¾“å…¥ç›®å½•ï¼Œè¦†ç›–é…ç½®
        if args.input_dir:
            config.paths.input_dir = args.input_dir

        config.setup_directories()

        # è®¾ç½®æ—¥å¿—
        log_file = config.paths.logs_dir / "unified_batch_processor.log"
        logger = setup_logging(log_file, args.log_level)

        logger.info("=" * 50)
        logger.info("ç»Ÿä¸€æ‰¹å¤„ç†å…¥å£")
        logger.info("=" * 50)

        # åˆ›å»ºå¤„ç†å™¨
        processor = UnifiedBatchProcessor(config)

        # è¿è¡Œå¤„ç†
        if args.mode == "pdf_only":
            results = processor.process_pdfs(limit=args.limit, workers=args.workers)
        elif args.mode == "parse_only":
            results = processor.parse_mds(limit=args.limit)
        elif args.mode == "import_only":
            results = processor.import_data(limit=args.limit)
        else:  # full
            results = processor.run_full_pipeline(limit=args.limit, workers=args.workers)

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        if args.output_report:
            processor.generate_performance_report(args.output_report)
        else:
            processor.generate_performance_report()

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 50)
        print("æ‰§è¡Œç»“æœ:")
        print("=" * 50)

        if results.get('success', True):
            print("âœ… å¤„ç†æˆåŠŸ")
        else:
            print("âŒ å¤„ç†å¤±è´¥")

        if results.get('pdf_processing'):
            pdf = results['pdf_processing']
            print(f"ğŸ“„ PDFå¤„ç†: {pdf.get('processed', 0)} æˆåŠŸ, {pdf.get('failed', 0)} å¤±è´¥")

        if results.get('md_parsing'):
            md = results['md_parsing']
            print(f"ğŸ“ MDè§£æ: {md.get('parsed', 0)} æˆåŠŸ, {md.get('failed', 0)} å¤±è´¥")

        if results.get('data_import'):
            imp = results['data_import']
            print(f"ğŸ’¾ æ•°æ®å¯¼å…¥: {imp.get('imported', 0)} æˆåŠŸ, {imp.get('failed', 0)} å¤±è´¥")

        if results.get('error'):
            print(f"â— é”™è¯¯: {results['error']}")

        print("=" * 50)

        return 0 if results.get('success', True) else 1

    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 130
    except Exception as e:
        print(f"\nâŒ è‡´å‘½é”™è¯¯: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())